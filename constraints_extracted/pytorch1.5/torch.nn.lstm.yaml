check_nan: true
constraints:
  '**kwargs':
    descp: ''
  '*args':
    descp: ''
  batch_first:
    default: None
    descp: 'If `True`, then the input and output tensors are provided as (batch, seq,
      feature). Default: `False`'
    dtype:
    - torch.bool
    ndim:
    - '0'
    tensor_t:
    - torch.tensor
  bias:
    default: None
    descp: 'If `False`, then the layer does not use bias weights b_ih and b_hh. Default:
      `True`'
    dtype:
    - torch.bool
    ndim:
    - '0'
  bidirectional:
    default: None
    descp: 'If `True`, becomes a bidirectional LSTM. Default: `False`'
    dtype:
    - torch.bool
    ndim:
    - '0'
  dropout:
    default: None
    descp: 'If non-zero, introduces a Dropout layer on the outputs of each LSTM layer
      except the last layer, with dropout probability equal to `dropout`. Default:
      0'
    dtype:
    - int
    ndim:
    - '0'
  hidden_size:
    default: None
    descp: The number of features in the hidden state h
    dtype:
    - int
    ndim:
    - '0'
    range:
    - '[0,inf)'
  input_size:
    default: None
    descp: The number of expected features in the input x
    dtype:
    - int
    ndim:
    - '0'
    range:
    - '[0,inf)'
  num_layers:
    default: None
    descp: 'Number of recurrent layers. E.g., setting `num_layers=2` would mean stacking
      two LSTMs together to form a stacked LSTM, with the second LSTM taking in outputs
      of the first LSTM and computing the final results. Default: 1'
    dtype:
    - int
    ndim:
    - '0'
    range:
    - '[0,inf)'
inputs:
  optional:
  - input_size
  - hidden_size
  - num_layers
  - bias
  - batch_first
  - dropout
  - bidirectional
  required:
  - '*args'
  - '**kwargs'
layer_constructor: true
link: https://pytorch.org/docs/1.5.0/nn.html#torch.nn.LSTM
package: torch
target: LSTM
title: torch.nn.LSTM
version: 1.5.0
