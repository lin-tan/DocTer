check_nan: true
constraints:
  input:
    descp: Tensor containing indices into the embedding matrix
    doc_dtype: LongTensor
    dtype:
    - torch.int64
    tensor_t:
    - torch.tensor
  max_norm:
    default: None
    descp: 'If given, each embedding vector with norm larger than `max_norm` is renormalized
      to have norm `max_norm`. Note: this will modify `weight` in-place.'
    doc_dtype: float, optional
    dtype:
    - torch.float32
  norm_type:
    default: '2.0'
    descp: The p of the p-norm to compute for the `max_norm` option. Default `2`.
    doc_dtype: float, optional
    dtype:
    - int
    - torch.bool
    - torch.float32
    ndim:
    - '0'
  padding_idx:
    default: None
    descp: If given, pads the output with the embedding vector at `padding_idx` (initialized
      to zeros) whenever it encounters the index.
    doc_dtype: int, optional
    dtype:
    - int
    structure:
    - list
  scale_grad_by_freq:
    default: 'False'
    descp: If given, this will scale gradients by the inverse of frequency of the
      words in the mini-batch. Default `False`.
    doc_dtype: boolean, optional
    dtype:
    - torch.bool
    ndim:
    - '0'
  sparse:
    default: 'False'
    descp: If `True`, gradient w.r.t. `weight` will be a sparse tensor. See Notes
      under `torch.nn.Embedding` for more details regarding sparse gradients.
    doc_dtype: bool, optional
    dtype:
    - torch.bool
    ndim:
    - '0'
    tensor_t:
    - torch.tensor
  weight:
    descp: The embedding matrix with number of rows equal to the maximum possible
      index + 1, and number of columns equal to the embedding size
    doc_dtype: Tensor
    dtype:
    - int
    ndim:
    - '0'
    range:
    - '[0,inf)'
    tensor_t:
    - torch.tensor
inputs:
  optional:
  - padding_idx
  - max_norm
  - norm_type
  - scale_grad_by_freq
  - sparse
  required:
  - input
  - weight
link: https://pytorch.org/docs/1.5.0/nn.functional.html#torch.nn.functional.embedding
package: torch
target: embedding
title: torch.nn.functional.embedding
version: 1.5.0
