constraints:
  _caller:
    default: fork_rng
    descp: ''
    dtype:
    - string
  _devices_kw:
    default: devices
    descp: ''
    dtype:
    - string
  devices:
    default: None
    descp: CUDA devices for which to fork the RNG.  CPU RNG state is always forked.  By
      default, `fork_rng()` operates on all devices, but will emit a warning if your
      machine has a lot of devices, since this function will run very slowly in that
      case. If you explicitly specify devices, this warning will be suppressed
    doc_dtype: iterable of CUDA IDs
    structure:
    - list
  enabled:
    default: 'True'
    descp: if `False`, the RNG is not forked.  This is a convenience argument for
      easily disabling the context manager without having to delete it and unindent
      your Python code under it.
    doc_dtype: bool
    dtype:
    - torch.bool
    ndim:
    - '0'
inputs:
  optional:
  - devices
  - enabled
  - _caller
  - _devices_kw
  required: []
link: https://pytorch.org/docs/1.5.0/random.html#torch.random.fork_rng
package: torch
target: fork_rng
title: torch.random.fork_rng
version: 1.5.0
