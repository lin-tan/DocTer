constraints:
  allow_unused:
    default: 'False'
    descp: If `False`, specifying inputs that were not used when computing outputs
      (and therefore their grad is always zero) is an error. Defaults to `False`.
    doc_dtype: bool, optional
    dtype:
    - torch.bool
    ndim:
    - '0'
  create_graph:
    default: 'False'
    descp: 'If `True`, graph of the derivative will be constructed, allowing to compute
      higher order derivative products. Default: `False`.'
    doc_dtype: bool, optional
    dtype:
    - torch.bool
    ndim:
    - '0'
  grad_outputs:
    default: None
    descp: 'The "vector" in the Jacobian-vector product. Usually gradients w.r.t.
      each output. None values can be specified for scalar Tensors or ones that don''t
      require grad. If a None value would be acceptable for all grad_tensors, then
      this argument is optional. Default: None.'
    doc_dtype: sequence of Tensor
    enum:
    - vector
    structure:
    - list
    tensor_t:
    - torch.tensor
  inputs:
    descp: Inputs w.r.t. which the gradient will be returned (and not accumulated
      into `.grad`).
    doc_dtype: sequence of Tensor
    structure:
    - list
    tensor_t:
    - torch.tensor
  only_inputs:
    default: 'True'
    descp: ''
    dtype:
    - torch.bool
    ndim:
    - '0'
  outputs:
    descp: outputs of the differentiated function.
    doc_dtype: sequence of Tensor
    structure:
    - list
    tensor_t:
    - torch.tensor
  retain_graph:
    default: None
    descp: If `False`, the graph used to compute the grad will be freed. Note that
      in nearly all cases setting this option to `True` is not needed and often can
      be worked around in a much more efficient way. Defaults to the value of `create_graph`.
    doc_dtype: bool, optional
    dtype:
    - torch.bool
    ndim:
    - '0'
inputs:
  optional:
  - grad_outputs
  - retain_graph
  - create_graph
  - only_inputs
  - allow_unused
  required:
  - outputs
  - inputs
link: https://pytorch.org/docs/1.5.0/autograd.html#torch.autograd.grad
package: torch
target: grad
title: torch.autograd.grad
version: 1.5.0
