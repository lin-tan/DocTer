check_nan: true
constraints:
  '**kwargs':
    descp: ''
  '*args':
    descp: ''
  batch_first:
    default: None
    descp: 'If `True`, then the input and output tensors are provided as (batch, seq,
      feature). Default: `False`'
    dtype:
    - torch.bool
    ndim:
    - '0'
    tensor_t:
    - torch.tensor
  bias:
    default: None
    descp: 'If `False`, then the layer does not use bias weights b_ih and b_hh. Default:
      `True`'
    dtype:
    - torch.bool
    ndim:
    - '0'
  bidirectional:
    default: None
    descp: 'If `True`, becomes a bidirectional RNN. Default: `False`'
    dtype:
    - torch.bool
    ndim:
    - '0'
  dropout:
    default: None
    descp: 'If non-zero, introduces a Dropout layer on the outputs of each RNN layer
      except the last layer, with dropout probability equal to `dropout`. Default:
      0'
    dtype:
    - int
    ndim:
    - '0'
  hidden_size:
    default: None
    descp: The number of features in the hidden state h
    dtype:
    - int
    ndim:
    - '0'
    range:
    - '[0,inf)'
  input_size:
    default: None
    descp: The number of expected features in the input x
    dtype:
    - int
    ndim:
    - '0'
    range:
    - '[0,inf)'
  nonlinearity:
    default: None
    descp: 'The non-linearity to use. Can be either `''tanh''` or `''relu''`. Default:
      `''tanh''`'
    dtype:
    - torch.bool
    ndim:
    - '0'
  num_layers:
    default: None
    descp: 'Number of recurrent layers. E.g., setting `num_layers=2` would mean stacking
      two RNNs together to form a stacked RNN, with the second RNN taking in outputs
      of the first RNN and computing the final results. Default: 1'
    dtype:
    - int
    ndim:
    - '0'
    range:
    - '[0,inf)'
inputs:
  optional:
  - input_size
  - hidden_size
  - num_layers
  - nonlinearity
  - bias
  - batch_first
  - dropout
  - bidirectional
  required:
  - '*args'
  - '**kwargs'
layer_constructor: true
link: https://pytorch.org/docs/1.5.0/nn.html#torch.nn.RNN
package: torch
target: RNN
title: torch.nn.RNN
version: 1.5.0
